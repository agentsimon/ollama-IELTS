<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>IELTS Speaking Practice</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
      body {
        font-family: "Inter", sans-serif;
        background-color: #f3f4f6;
      }
      .container {
        max-width: 800px;
      }
      .disabled-style {
        opacity: 0.6;
        cursor: not-allowed;
        pointer-events: none;
        filter: grayscale(100%);
      }
    </style>
  </head>
  <body class="bg-gray-100 min-h-screen flex items-center justify-center p-4">
    <div class="container bg-white shadow-xl rounded-2xl p-8 space-y-8">
      <header class="text-center">
        <h1 class="text-3xl font-bold text-gray-800">
          IELTS Speaking 1 Practice
        </h1>
        <p class="text-lg text-gray-600 mt-2">
          Practice for your IELTS speaking exam with a virtual tutor powered by
          a local LLM.
        </p>
        <div class="flex justify-center gap-4 mt-4">
          <a
            href="speaking2.html"
            class="bg-blue-600 text-white font-bold py-3 px-6 rounded-full shadow-lg hover:bg-blue-700 transition duration-300 transform hover:scale-105"
          >
            Speaking 2
          </a>
          <a
            href="speaking3.html"
            class="bg-blue-600 text-white font-bold py-3 px-6 rounded-full shadow-lg hover:bg-blue-700 transition duration-300 transform hover:scale-105"
          >
            Speaking 3
          </a>
        </div>
      </header>

      <div id="topic-info" class="text-center text-gray-700 font-medium">
        <span id="topic-name"></span> | <span id="question-count"></span>
      </div>

      <div class="space-y-4">
        <p
          id="question-display"
          class="bg-blue-50 p-6 rounded-lg shadow-inner border-l-4 border-blue-500 text-xl text-blue-800 text-center font-semibold"
        >
          Press 'Next Question' to hear the next question.
        </p>
        <div
          id="status-message"
          class="text-center text-sm font-medium text-gray-500"
        ></div>
      </div>

      <div id="mic-level-container" class="space-y-2 hidden">
        <p class="text-center text-sm font-medium text-gray-600">
          Microphone Level
        </p>
        <div
          class="w-full h-8 bg-gray-200 rounded-full overflow-hidden shadow-inner"
        >
          <div
            id="mic-level-bar"
            class="h-full rounded-full transition-all duration-75 ease-out"
            style="
              width: 0%;
              background-image: linear-gradient(
                to right,
                #4ade80,
                #facc15,
                #ef4444
              );
            "
          ></div>
        </div>
      </div>

      <div class="flex flex-col sm:flex-row justify-center items-center gap-4">
        <button
          id="next-question-btn"
          class="bg-blue-600 text-white font-bold py-3 px-6 rounded-full shadow-lg hover:bg-blue-700 transition duration-300 transform hover:scale-105"
        >
          Next Question
        </button>
        <button
          id="start-listen-btn"
          class="bg-green-600 text-white font-bold py-3 px-6 rounded-full shadow-lg hover:bg-green-700 transition duration-300 transform hover:scale-105"
          disabled
        >
          Start Answer
        </button>
        <button
          id="stop-listen-btn"
          class="bg-red-600 text-white font-bold py-3 px-6 rounded-full shadow-lg hover:bg-red-700 transition duration-300 transform hover:scale-105"
          disabled
        >
          Stop Listening
        </button>
      </div>

      <div id="result-container" class="space-y-6">
        <div class="bg-gray-50 p-6 rounded-lg shadow">
          <h2 class="text-xl font-bold text-gray-700 mb-2">
            Your Answer Transcript:
          </h2>
          <div
            id="transcription-display"
            class="text-gray-900 text-lg border p-4 rounded-lg bg-white min-h-[50px]"
          ></div>
        </div>

        <div class="bg-gray-50 p-6 rounded-lg shadow">
          <h2 class="text-xl font-bold text-gray-700 mb-2">
            Assessment from Llama 3:
          </h2>
          <div
            id="assessment-result"
            class="text-gray-900 text-lg border p-4 rounded-lg bg-white min-h-[100px] whitespace-pre-wrap"
          >
            <div
              id="loading-spinner"
              class="hidden flex justify-center items-center h-full"
            >
              <div
                class="animate-spin rounded-full h-10 w-10 border-4 border-blue-500 border-t-transparent"
              ></div>
            </div>
          </div>
        </div>
      </div>

      <div
        id="modal"
        class="fixed inset-0 bg-gray-600 bg-opacity-50 hidden items-center justify-center p-4"
      >
        <div
          class="bg-white rounded-lg shadow-xl p-6 w-full max-w-sm text-center"
        >
          <p id="modal-message" class="text-lg font-medium text-gray-800"></p>
          <button
            onclick="closeModal()"
            class="mt-4 bg-blue-600 text-white py-2 px-4 rounded-full hover:bg-blue-700 transition"
          >
            OK
          </button>
        </div>
      </div>
    </div>

    <script>
      // Use a self-contained JSON data object for the topics and questions.
      // In a production environment, you would fetch this from a file.
      const topics = [
        {
          topic: "Mobile Phones",
          questions: [
            "Do you have a mobile phone?",
            "At what age did you first get a mobile?",
            "What do you most use it for?",
            "Is it a nuisance if people use mobiles in public places such as trains and buses?",
          ],
        },
        {
          topic: "Reading",
          questions: [
            "Do you enjoy reading?",
            "What kind of books do you read most often?",
            "Do you think reading is a good way to relax?",
            "How has reading changed in the digital age?",
          ],
        },
        {
          topic: "Hometown",
          questions: [
            "Where are you from?",
            "What do you like most about your hometown?",
            "What are the main places of interest in your hometown?",
            "Has your hometown changed much since you were a child?",
          ],
        },
        {
          topic: "Sports",
          questions: [
            "Do you like to play or watch sports?",
            "What sports are popular in your country?",
            "What are the benefits of playing sports?",
            "Did you play any sports when you were a child?",
          ],
        },
      ];

      // --- DOM Elements ---
      const nextQuestionBtn = document.getElementById("next-question-btn");
      const startBtn = document.getElementById("start-listen-btn");
      const stopBtn = document.getElementById("stop-listen-btn");
      const questionDisplay = document.getElementById("question-display");
      const transcriptionDisplay = document.getElementById(
        "transcription-display"
      );
      const assessmentResult = document.getElementById("assessment-result");
      const statusMessage = document.getElementById("status-message");
      const resultContainer = document.getElementById("result-container");
      const loadingSpinner = document.getElementById("loading-spinner");
      const micLevelBar = document.getElementById("mic-level-bar");
      const micLevelContainer = document.getElementById("mic-level-container");
      const topicNameElement = document.getElementById("topic-name");
      const questionCountElement = document.getElementById("question-count");

      // --- Global State ---
      let availableTopics = [...topics];
      let currentTopic = null;
      let currentQuestionIndex = 0;
      let animationFrameId = null;

      // --- Web Speech API Setup ---
      const synth = window.speechSynthesis;
      const SpeechRecognition =
        window.SpeechRecognition || window.webkitSpeechRecognition;
      let recognition = null;
      if (SpeechRecognition) {
        recognition = new SpeechRecognition();
        recognition.continuous = false; // Stop listening after a pause
        recognition.interimResults = false; // Only return final results
        recognition.lang = "en-US";
      } else {
        showModal(
          "Speech Recognition is not supported by your browser. Please use Chrome or Edge."
        );
        startBtn.disabled = true;
        nextQuestionBtn.disabled = true;
      }

      // --- Web Audio API Setup for visualizer ---
      let audioContext, analyser, microphone, dataArray;

      /**
       * Sets up the microphone visualizer using the Web Audio API.
       * Requests microphone access and connects the audio stream to an analyser.
       */
      async function setupMicrophoneVisualizer() {
        try {
          const stream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });
          audioContext = new (window.AudioContext ||
            window.webkitAudioContext)();
          analyser = audioContext.createAnalyser();
          microphone = audioContext.createMediaStreamSource(stream);

          analyser.fftSize = 256;
          dataArray = new Uint8Array(analyser.frequencyBinCount);

          // Connect the microphone to the analyser
          microphone.connect(analyser);
        } catch (err) {
          console.error("Error accessing microphone:", err);
          showModal(
            "Microphone access denied. Please allow microphone access to use the visualizer and speech recognition."
          );
          startBtn.disabled = true;
        }
      }

      /**
       * The main animation loop for the microphone level indicator.
       */
      function updateMicLevel() {
        analyser.getByteFrequencyData(dataArray);

        // Calculate the average volume
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) {
          sum += dataArray[i];
        }
        const average = sum / dataArray.length;

        // Normalize the value to a percentage (0-100) and update the bar width
        const normalizedValue = Math.min(100, (average / 128) * 100);
        micLevelBar.style.width = `${normalizedValue}%`;

        animationFrameId = requestAnimationFrame(updateMicLevel);
      }

      /**
       * Starts the microphone visualizer animation.
       */
      function startVisualizer() {
        if (analyser) {
          micLevelContainer.classList.remove("hidden");
          animationFrameId = requestAnimationFrame(updateMicLevel);
        }
      }

      /**
       * Stops the microphone visualizer animation.
       */
      function stopVisualizer() {
        if (animationFrameId) {
          cancelAnimationFrame(animationFrameId);
          micLevelBar.style.width = "0%";
          micLevelContainer.classList.add("hidden");
        }
      }

      // --- Ollama API Setup ---
      const ollamaEndpoint = "http://localhost:11434/api/chat";
      const ollamaModel = "llama3";

      // The detailed IELTS speaking band descriptors provided by the user
      const ieltsPromptGuide = `You are an expert IELTS Speaking examiner. Your task is to provide a detailed assessment of a user's spoken answer based on the following criteria.
        
        **Assessment Criteria:**
        **Fluency and Coherence:**
        Band 9: Speaks fluently with only rare repetition or self-correction; any hesitation is content-related. Speaks coherently with fully appropriate cohesive features. Develops topics fully and appropriately.
        Band 8: Speaks fluently with only occasional repetition or self-correction; hesitation is usually content-related. Develops topics coherently and appropriately.
        Band 7: Speaks at length without noticeable effort or loss of coherence. May demonstrate language-related hesitation at times. Uses a range of connectives with some flexibility.
        Band 6: Willing to speak at length, though may lose coherence at times due to occasional repetition or hesitation. Uses a range of connectives but not always appropriately.
        Band 5: Usually maintains flow of speech but uses repetition, self correction and/or slow speech to keep going. May over-use certain connectives.
        Band 4: Cannot respond without noticeable pauses and may speak slowly, with frequent repetition and self-correction.
        Band 3: Speaks with long pauses. Has limited ability to link simple sentences. Gives only simple responses.
        Band 2: Pauses lengthily before most words. Little communication possible.
        Band 1: No communication possible. No rateable language.

        **Lexical Resource:**
        Band 9: Uses vocabulary with full flexibility and precision in all topics. Uses idiomatic language naturally and accurately.
        Band 8: Uses a wide vocabulary readily and flexibly. Uses less common and idiomatic vocabulary skilfully, with occasional inaccuracies. Uses paraphrase effectively.
        Band 7: Uses vocabulary flexibly to discuss a variety of topics. Uses some less common and idiomatic vocabulary. Uses paraphrase effectively.
        Band 6: Has a wide enough vocabulary to discuss topics at length and make meaning clear in spite of inappropriacies. Generally paraphrases successfully.
        Band 5: Manages to talk about familiar and unfamiliar topics but uses vocabulary with limited flexibility. Attempts to use paraphrase but with mixed success.
        Band 4: Is able to talk about familiar topics but can only convey basic meaning on unfamiliar topics and makes frequent errors in word choice. Rarely attempts paraphrase.
        Band 3: Uses simple vocabulary to convey personal information. Has insufficient vocabulary for less familiar topics.
        Band 2: Only produces isolated words or memorised utterances.
        Band 1: No communication possible. No rateable language.

        **Grammatical Range and Accuracy:**
        Band 9: Uses a full range of structures naturally and appropriately. Produces consistently accurate structures apart from ‘slips’.
        Band 8: Uses a wide range of structures flexibly. Produces a majority of error-free sentences with only very occasional inappropriacies or basic/non-systematic errors.
        Band 7: Uses a range of complex structures with some flexibility. Frequently produces error-free sentences, though some grammatical mistakes persist.
        Band 6: Uses a mix of simple and complex structures, but with limited flexibility. May make frequent mistakes with complex structures.
        Band 5: Produces basic sentence forms with reasonable accuracy. Uses a limited range of more complex structures, but these usually contain errors.
        Band 4: Produces basic sentence forms and some correct simple sentences but subordinate structures are rare. Errors are frequent.
        Band 3: Attempts basic sentence forms but with limited success, or relies on memorised utterances. Makes numerous errors.
        Band 2: Cannot produce basic sentence forms.
        Band 1: No communication possible. No rateable language.

        **Pronunciation:**
        Band 9: Uses a full range of pronunciation features with precision and subtlety. Sustains flexible use of features throughout. Is effortless to understand.
        Band 8: Uses a wide range of pronunciation features. Sustains flexible use of features, with only occasional lapses. Is easy to understand throughout; L1 accent has minimal effect on intelligibility.
        Band 7: Shows all the positive features of Band 6 and some, but not all, of the positive features of Band 8.
        Band 6: Uses a range of pronunciation features with mixed control. Can generally be understood throughout.
        Band 5: Shows all features of band 4 and some, but not all the positive features of band 6.
        Band 4: Uses a limited range of pronunciation features. Mispronunciations are frequent and cause some difficulty for the listener.
        Band 3: Shows some of the features of band 2 and some, but not all, of the positive features of band 4.
        Band 2: Speech is often unintelligible.
        Band 1: No communication possible. No rateable language.

        Your assessment must be formatted as follows, providing a score for each criterion and an overall band score. You must also provide specific, actionable advice for improvement based on the transcript.

        **Assessment:**
        **Fluency and Coherence:** [Band Score]
        **Lexical Resource:** [Band Score]
        **Grammatical Range and Accuracy:** [Band Score]
        **Pronunciation:** [Band Score]
        **Overall Band Score:** [Overall Score]

        **Advice:**
        [Detailed, specific advice on how to improve]`;

      // Now, please assess the following transcript based on the user's answer to the question: "${currentQuestion}"

      // User's transcript: "${transcript}"`;

      // --- Functions ---

      /**
       * Custom modal to display messages instead of alert().
       * @param {string} message The message to display.
       */
      function showModal(message) {
        const modal = document.getElementById("modal");
        const modalMessage = document.getElementById("modal-message");
        modalMessage.textContent = message;
        modal.style.display = "flex";
      }

      /**
       * Closes the custom modal.
       */
      function closeModal() {
        const modal = document.getElementById("modal");
        modal.style.display = "none";
      }

      /**
       * Updates the state of the control buttons.
       * @param {boolean} next Whether the next question button is enabled.
       * @param {boolean} start Whether the start listening button is enabled.
       * @param {boolean} stop Whether the stop listening button is enabled.
       */
      function updateButtons(next, start, stop) {
        nextQuestionBtn.disabled = !next;
        startBtn.disabled = !start;
        stopBtn.disabled = !stop;

        // Add or remove the visual disabled style and grayscale effect
        nextQuestionBtn.classList.toggle("disabled-style", !next);
        startBtn.classList.toggle("disabled-style", !start);
        stopBtn.classList.toggle("disabled-style", !stop);
      }

      /**
       * Reads the next random question from the available topics and speaks it aloud.
       */
      function readQuestion() {
        // Check if all questions from a topic have been asked
        if (
          !currentTopic ||
          currentQuestionIndex >= currentTopic.questions.length
        ) {
          if (availableTopics.length === 0) {
            showModal(
              "All topics have been covered. Please refresh the page to start again."
            );
            updateButtons(false, false, false);
            questionDisplay.textContent = "Session Complete!";
            topicNameElement.textContent = "";
            questionCountElement.textContent = "";
            return;
          }
          const randomIndex = Math.floor(
            Math.random() * availableTopics.length
          );
          currentTopic = availableTopics.splice(randomIndex, 1)[0];
          currentQuestionIndex = 0;
          topicNameElement.textContent = `Topic: ${currentTopic.topic}`;
        }

        const question = currentTopic.questions[currentQuestionIndex];
        // Update the question count display
        questionCountElement.textContent = `Question ${
          currentQuestionIndex + 1
        } of ${currentTopic.questions.length}`;

        // Display a temporary message while the question is being read
        questionDisplay.textContent = "Question is being read...";
        transcriptionDisplay.textContent = "";
        assessmentResult.textContent = "";
        statusMessage.textContent = "Please wait for the question to finish...";
        updateButtons(false, false, false);

        const utterance = new SpeechSynthesisUtterance(question);
        utterance.lang = "en-US";
        utterance.onend = () => {
          // Clear the question text after it has been spoken
          questionDisplay.textContent =
            currentTopic.questions[currentQuestionIndex - 1]; // Display the final question text
          // Enable the Start Answer button after the question is read
          updateButtons(false, true, false);
          statusMessage.textContent =
            "Ready to record. Press 'Start Answer' when you are ready.";
        };
        synth.speak(utterance);
        currentQuestionIndex++;
      }

      /**
       * Starts the speech recognition process.
       */
      function startListening() {
        if (!recognition) return;
        recognition.start();
        startVisualizer();
        statusMessage.textContent =
          "Listening... Press 'Stop Listening' when you're done.";
        updateButtons(false, false, true);
      }

      /**
       * Stops the speech recognition process.
       */
      function stopListening() {
        if (!recognition) return;
        recognition.stop();
        stopVisualizer();
        statusMessage.textContent = "Processing your answer...";
        updateButtons(false, false, false);
        loadingSpinner.classList.remove("hidden");
        assessmentResult.textContent = ""; // Clear previous results
      }

      /**
       * Sends the user's transcribed text to the local Ollama model for assessment.
       * @param {string} transcript The transcribed text from the user's speech.
       */
      async function sendToOllama(transcript) {
        const currentQuestion = questionDisplay.textContent;

        const prompt = `${ieltsPromptGuide}
            
            Now, please assess the following transcript based on the user's answer to the question: "${currentQuestion}"
            
            User's transcript: "${transcript}"`;

        try {
          const response = await fetch(ollamaEndpoint, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify({
              model: ollamaModel,
              messages: [
                {
                  role: "user",
                  content: prompt,
                },
              ],
              stream: false, // We need the full response
            }),
          });

          if (!response.ok) {
            throw new Error(`HTTP error! Status: ${response.status}`);
          }

          const data = await response.json();
          const assessment = data.message.content;

          loadingSpinner.classList.add("hidden");
          assessmentResult.textContent = assessment;
        } catch (error) {
          console.error("Error fetching from Ollama:", error);
          loadingSpinner.classList.add("hidden");
          showModal(
            "Error connecting to Ollama. Please ensure your Ollama server is running locally and the model 'llama3' is installed."
          );
        } finally {
          // Re-enable the "Next Question" button for the next round
          updateButtons(true, false, false);
          statusMessage.textContent =
            "Assessment complete. Press 'Next Question' for the next one.";
        }
      }

      // --- Event Listeners ---
      nextQuestionBtn.addEventListener("click", () => {
        readQuestion();
      });

      startBtn.addEventListener("click", () => {
        transcriptionDisplay.textContent = "";
        resultContainer.classList.remove("hidden");
        startListening();
      });

      stopBtn.addEventListener("click", stopListening);

      if (recognition) {
        recognition.onresult = (event) => {
          let interimTranscript = "";
          let finalTranscript = "";
          for (let i = event.resultIndex; i < event.results.length; ++i) {
            const transcript = event.results[i][0].transcript;
            if (event.results[i].isFinal) {
              finalTranscript += transcript;
            } else {
              interimTranscript += transcript;
            }
          }
          // Display the final transcript once the user has stopped speaking.
          transcriptionDisplay.textContent = finalTranscript;
        };

        recognition.onend = () => {
          if (transcriptionDisplay.textContent) {
            sendToOllama(transcriptionDisplay.textContent);
          } else {
            loadingSpinner.classList.add("hidden");
            updateButtons(true, false, false);
            statusMessage.textContent =
              "No speech detected. Press 'Next Question' to continue.";
          }
        };

        recognition.onerror = (event) => {
          console.error("Speech recognition error:", event.error);
          stopBtn.disabled = true;
          showModal(`Speech recognition error: ${event.error}`);
          updateButtons(true, false, false);
        };
      }

      // Initial setup
      updateButtons(true, false, false);
      // Set up the microphone visualizer on load
      setupMicrophoneVisualizer();
    </script>
  </body>
</html>
